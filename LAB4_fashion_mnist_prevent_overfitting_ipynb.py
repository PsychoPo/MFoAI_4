# -*- coding: utf-8 -*-
"""Копия блокнота "fashion_mnist_prevent_overfitting.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1axSNfDHwOTdj80Y_dBU86iIHcHr4gQRL

# Анализ качества работы нейронной сети для распознавания моделей одежды в Keras

Чтобы запускать и редактировать код, сохраните копию этого ноутбука себе (File->Save a copy in Drive...). Свою копию вы сможете изменять и запускать.

Учебный курс "[Программирование нейросетей на Python](https://www.asozykin.ru/courses/nnpython)".
"""

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import utils
from tensorflow.keras.preprocessing import image
from google.colab import files
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
# %matplotlib inline

"""## Подготовка данных для обучения сети

**Загружаем набор данных**
"""

# В Keras встроены средства работы с популярными наборами данных
# (x_train, y_train) - набор данных для обучения
# (x_test, y_test) - набор данных для тестирования
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

"""Список с названиями классов"""

classes = ['футболка', 'брюки', 'свитер', 'платье', 'пальто', 'туфли', 'рубашка', 'кроссовки', 'сумка', 'ботинки']

"""Просматриваем примеры изображений"""

plt.figure(figsize=(10,10))
for i in range(100,150):
    plt.subplot(5,10,i-100+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    plt.xlabel(classes[y_train[i]])

"""**Преобразование размерности данных в наборе**"""

x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)

"""**Нормализация данных**"""

# Векторизованные операции
# Применяются к каждому элементу массива отдельно
x_train = x_train / 255
x_test = x_test / 255

"""**Работа с правильными ответами**"""

n = 0

print(y_train[n])

"""**Преобразуем метки в формат one hot encoding**"""

y_train = utils.to_categorical(y_train, 10)

y_test = utils.to_categorical(y_test, 10)

"""**Правильный ответ в формате one hot encoding**"""

print(y_train[n])

"""## Создаем нейронную сеть

**Создаем последовательную модель**
"""

# Создаем последовательную модель
model = Sequential()
# Входной полносвязный слой, 800 нейронов, 784 входа в каждый нейрон
model.add(Dense(800, input_dim=784, activation="relu"))
# Выходной полносвязный слой, 10 нейронов (по количеству рукописных цифр)
model.add(Dense(10, activation="softmax"))

"""**Компилируем сеть**"""

model.compile(loss="categorical_crossentropy", optimizer="SGD", metrics=["accuracy"])

print(model.summary())

"""## Обучаем нейронную сеть"""

history = model.fit(x_train, y_train,
                    batch_size=200,
                    epochs=100,
                    validation_split=0.2,
                    verbose=1)

# Графики Задание №1
history.history.get('val_accuracy')
history.epoch
plt.title("Взаимосвязь числа эпох и валидации на тренировочном наборе данных")
plt.xlabel('Эпохи')
plt.ylabel('Валидация')
plt.plot(history.epoch, history.history.get('accuracy'),color='r', label='train')
plt.show()

plt.title("Взаимосвязь числа эпох и валидации на тестовом наборе данных")
plt.xlabel('Эпохи')
plt.ylabel('Валидация')
plt.plot(history.epoch, history.history.get('val_accuracy'),color='b', label='test')
plt.show()

# Созместное сравнение графиков
plt.plot(history.epoch, history.history.get('accuracy'),color='r', label='train')
plt.plot(history.epoch, history.history.get('val_accuracy'),color='b', label='test')
plt.xlabel('Эпохи')
plt.ylabel('Валидация')
plt.title("Сравнение взаимосвязи числа эпох и валидации на тестовом и тренировочном наборах данных")
plt.legend()
plt.show()

# Созместное сравнение графиков
plt.plot(history.epoch, history.history.get('loss'),color='r', label='train')
plt.plot(history.epoch, history.history.get('val_loss'),color='b', label='test')
plt.xlabel('Эпохи')
plt.ylabel('Потери(ошибки)')
plt.title("Сравнение взаимосвязи числа эпох и потерь(ошибок) на тестовом и тренировочном наборах данных")
plt.legend()
plt.show()

# Изменяем структуру сети с целью улучшения качества задание № 2

# Создаем последовательную модель
model = Sequential()
# Наше изображение 28*28 следовательно минимум нейронов первого слоя 784
# Если добавим больше, то они до определённого момента будут повышать тосность
# Оставим прежнее значение 800, изменим же число слоёв

# Входной полносвязный слой, 800 нейронов, 784 входа в каждый нейрон
model.add(Dense(800, input_dim=784, activation="relu"))
# слой, 196 нейронов
model.add(Dense(196, activation="relu"))
# Выходной полносвязный слой, 10 нейронов (по количеству рукописных цифр)
model.add(Dense(10, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="SGD", metrics=["accuracy"])

print(model.summary())

history = model.fit(x_train, y_train,
                    batch_size=200,
                    epochs=100,
                    validation_split=0.2,
                    verbose=1)

#Стало лучше
# для сравнения
# итог при начальной модели: loss: 0.3690 - accuracy: 0.8733 - val_loss: 0.3926 - val_accuracy: 0.8616
# итог при модели с доп. слоем: loss: 0.2548 - accuracy: 0.9103 - val_loss: 0.3260 - val_accuracy: 0.8851
# Графики изменённой модели - валидация
plt.plot(history.epoch, history.history.get('accuracy'),color='r', label='train')
plt.plot(history.epoch, history.history.get('val_accuracy'),color='b', label='test')
plt.xlabel('Эпохи')
plt.ylabel('Валидация')
plt.title("Сравнение взаимосвязи числа эпох и валидации на тестовом и тренировочном наборах данных")
plt.legend()
plt.show()

# Графики изменённой модели - ошибки
plt.plot(history.epoch, history.history.get('loss'),color='r', label='train')
plt.plot(history.epoch, history.history.get('val_loss'),color='b', label='test')
plt.xlabel('Эпохи')
plt.ylabel('Потери(ошибки)')
plt.title("Сравнение взаимосвязи числа эпох и потерь(ошибок) на тестовом и тренировочном наборах данных")
plt.legend()
plt.show()

# Изменяем структуру сети с целью улучшения качества задание № 2

# Создаем последовательную модель
model = Sequential()
# Изменим функцию активации, сделаем sigmoid

# Входной полносвязный слой, 800 нейронов, 784 входа в каждый нейрон
model.add(Dense(800, input_dim=784, activation="sigmoid"))
# Выходной полносвязный слой, 10 нейронов (по количеству рукописных цифр)
model.add(Dense(10, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="SGD", metrics=["accuracy"])

print(model.summary())

history = model.fit(x_train, y_train,
                    batch_size=200,
                    epochs=100,
                    validation_split=0.2,
                    verbose=1)

#Стало хуже с функцией активации sigmoid
# для сравнения
# итог при начальной модели: loss: 0.3690 - accuracy: 0.8733 - val_loss: 0.3926 - val_accuracy: 0.8616
# итог при модели с ф. sigmoid: loss: 0.4513 - accuracy: 0.8447 - val_loss: 0.4573 - val_accuracy: 0.8389

# Изменяем структуру сети с целью улучшения качества задание № 2

# Создаем последовательную модель
model = Sequential()
# Изменим функцию активации, сделаем tanh

# Входной полносвязный слой, 800 нейронов, 784 входа в каждый нейрон
model.add(Dense(800, input_dim=784, activation="tanh"))
# Выходной полносвязный слой, 10 нейронов (по количеству рукописных цифр)
model.add(Dense(10, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="SGD", metrics=["accuracy"])

print(model.summary())

history = model.fit(x_train, y_train,
                    batch_size=200,
                    epochs=100,
                    validation_split=0.2,
                    verbose=1)

#Стало чуть лучше с функцией активации tanh
# для сравнения
# итог при начальной модели: loss: 0.3690 - accuracy: 0.8733 - val_loss: 0.3926 - val_accuracy: 0.8616
# итог при модели с ф. sigmoid: loss: 0.3476 - accuracy: 0.8777 - val_loss: 0.3771 - val_accuracy: 0.8662

# Теперь совместим функцию активации tanh с допю слоем 196

# Создаем последовательную модель
model = Sequential()

# Входной полносвязный слой, 800 нейронов, 784 входа в каждый нейрон
model.add(Dense(800, input_dim=784, activation="tanh"))
# слой, 196 нейронов
model.add(Dense(196, activation="tanh"))
# Выходной полносвязный слой, 10 нейронов (по количеству рукописных цифр)
model.add(Dense(10, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="SGD", metrics=["accuracy"])

print(model.summary())

history = model.fit(x_train, y_train,
                    batch_size=200,
                    epochs=100,
                    validation_split=0.2,
                    verbose=1)

#Стало лучше чем начальное, но хуже чем просто модель с допю слоем
# для сравнения
# итог при начальной модели: loss: 0.3690 - accuracy: 0.8733 - val_loss: 0.3926 - val_accuracy: 0.8616
# итог при модели с доп. слоем: loss: 0.2548 - accuracy: 0.9103 - val_loss: 0.3260 - val_accuracy: 0.8851
# итог при модели с доп. слоем и tanh: loss: 0.2936 - accuracy: 0.8955 - val_loss: 0.3374 - val_accuracy: 0.8769

# Итог лучше всего себя показала модель с доп. слоем

# Теперь преступим к заданию №3
# мы будем использовать оптимизатор Adam со скоростью обучения = 0.001 и функцией потерь как 'categorical_crossentropy'.
from tensorflow import keras

# Создаем последовательную модель
model = Sequential()

# Входной полносвязный слой, 800 нейронов, 784 входа в каждый нейрон
model.add(Dense(800, input_dim=784, activation="relu"))
# слой, 196 нейронов
model.add(Dense(196, activation="relu"))
# Выходной полносвязный слой, 10 нейронов (по количеству рукописных цифр)
model.add(Dense(10, activation="softmax"))

optimizer_new = keras.optimizers.Adam(learning_rate=0.001)
# мы будем использовать оптимизатор Adam со скоростью обучения = 0.001 и функцией потерь как 'categorical_crossentropy'.

model.compile(loss="categorical_crossentropy", optimizer=optimizer_new, metrics=["accuracy"])

print(model.summary())

history = model.fit(x_train, y_train,
                    batch_size=200,
                    epochs=100,
                    validation_split=0.2,
                    verbose=1)
# По итогу с оптимизатором Adam со скоростью обучения = 0.001 мы получили лучшие результаты
# Было : loss: 0.2548 - accuracy: 0.9103 - val_loss: 0.3260 - val_accuracy: 0.8851
# Стало : loss: 0.0237 - accuracy: 0.9913 - val_loss: 0.7714 - val_accuracy: 0.8993

"""## Сохраняем нейронную сеть для последующего использования"""

model.save('fashion_mnist_dense.h5')

"""## Оценка качества обучения

Проверка качества работы на наборе данных для тестирования
"""

scores = model.evaluate(x_test, y_test, verbose=1)

print("Доля верных ответов на тестовых данных, в процентах:", round(scores[1] * 100, 4))

"""## Используем сеть для распознавания предметов одежды"""

n_rec = 496

plt.imshow(x_test[n_rec].reshape(28, 28), cmap=plt.cm.binary)
plt.show()

"""**Меняем размерность изображения и нормализуем его**"""

x = x_test[n_rec]
x = np.expand_dims(x, axis=0)

"""**Запускаем распознавание**"""

prediction = model.predict(x)

"""**Печатаем результаты распознавания**"""

prediction

"""**Преобразуем результаты из формата one hot encoding**"""

prediction = np.argmax(prediction[0])
print("Номер класса:", prediction)
print("Название класса:", classes[prediction])

"""**Печатаем правильный ответ**"""

label = np.argmax(y_test[0])
print("Номер класса:", label)
print("Название класса:", classes[label])

"""## Загружаем свою картинку"""

files.upload()

"""Проверяем загрузку картинки"""

!ls

"""Загружаем картинку из файла"""

img_path = 'pic_3.jpg'
img = image.load_img(img_path, target_size=(28, 28), color_mode = "grayscale")
plt.imshow(image.load_img(img_path))

"""Показываем картинку"""

plt.imshow(img.convert('RGBA'))
plt.show()

"""Преобразуем картинку для обработки нейронной сетью"""

# Преобразуем картинку в массив
x = image.img_to_array(img)
# Меняем форму массива в плоский вектор
x = x.reshape(1, 784)
# Инвертируем изображение
x = 255 - x
# Нормализуем изображение
x /= 255

"""Запускаем распознавание"""

prediction = model.predict(x)

"""Результаты распознавания"""

prediction

prediction = np.argmax(prediction)
print("Номер класса:", prediction)
print("Название класса:", classes[prediction])

"""## Идеи по изменению архитектуры нейронной сети

1. Попробуйте использовать разное количество нейронов на входном слое: 400, 600, 800, 1200.
2. Добавьте в нейронную сеть скрытый слой с разным количеством нейронов: 200, 300, 400, 600, 800.
3. Добавьте несколько скрытых слоев в сеть с разным количеством нейронов в каждом слое.
3. Используйте разное количество эпох: 10, 15, 20, 25, 30.
4. Используйте разные размеры мини-выборки (batch_size): 10, 50, 100, 200, 500.


Подберить разные комбинации гиперпараметров таким образом, чтобы получить лучший результат на тестовом наборе данных.

Убедитесь, что в вашей модели нет переобучения.
"""